version: "3.8"

services:
  llm-extractor:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: llm-knowledge-extractor
    ports:
      - "8000:8000"
    environment:
      # LLM Configuration
      - LLM_CLIENT=${LLM_CLIENT:-mock}
      - OPENAI_API_KEY=${OPENAI_API_KEY:-}
      - OPENAI_MODEL=${OPENAI_MODEL:-gpt-4o-mini}
      - CLAUDE_API_KEY=${CLAUDE_API_KEY:-}
      - CLAUDE_MODEL=${CLAUDE_MODEL:-claude-3-haiku-20240307}
      - LLAMA_BASE_URL=${LLAMA_BASE_URL:-http://ollama:11434}
      - LLAMA_MODEL=${LLAMA_MODEL:-llama3.2:3b}

      # Database Configuration
      - DATABASE_URL=${DATABASE_URL:-sqlite:///./app.db}

      # Logging Configuration
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      - LOG_FILE=${LOG_FILE:-logs/app.log}
    volumes:
      - ./logs:/app/logs
      - ./data:/app/data
    depends_on:
      - ollama
    restart: unless-stopped
    networks:
      - llm-network

  # Ollama service for local LLM models
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0
    restart: unless-stopped
    networks:
      - llm-network
    # Uncomment the following lines to automatically pull a model on startup
    # command: >
    #   sh -c "ollama serve &
    #          sleep 10 &&
    #          ollama pull llama3.2:3b &&
    #          wait"

volumes:
  ollama_data:

networks:
  llm-network:
    driver: bridge
